
this is usualy due to endpoint problems like FW or mis-allignment between contrail and kubelet

test the api endpoint from each node.
 curl -k https://100.72.100.130:6443
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401

looks ok, so fw etc are ok. 

================================


check the cno file..

ls /var/lib/contrail/ports/vm/ is empty, so problem.. 

vrouter api looks ok
curl -X GET http://localhost:9091/port/753a65ef-918a-4193-9266-dc23ed8a5982
{"error":"{ Not Found }"}

looks fine

===============================

kubectl get services -A
NAMESPACE     NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes       ClusterIP   10.43.0.1      <none>        443/TCP                  3h3m
kube-system   kube-dns         ClusterIP   10.43.0.10     <none>        53/UDP,53/TCP,9153/TCP   3h3m
kube-system   metrics-server   ClusterIP   10.43.172.16   <none>        443/TCP                  3h3m


ps -ef | grep kubelet
--service-cluster-ip-range=10.43.0.0/16

contrail
       "k8sServiceBaseIP": { 10.96.0.0/24
        "k8sPodBaseIP": {  "Default": "10.32.0.0/24"
      "k8sFabricBaseIP": {"Default": "10.97.0.0/24"

aligned service subnet on contrail to 10.43.0.0/16

redeploy
same

add config for bgp and for link local as its not in yet. done need to redeploy
also add test pods, so we can try something other than coredns
test pod also gets no ip.

describe the failing test pod..

Events:
  Type     Reason                  Age                From               Message
  ----     ------                  ----               ----               -------
  Normal   Scheduled               2m46s              default-scheduler  Successfully assigned default/debug-default-67768587f8-7ck9h to rnode-03
  Warning  FailedCreatePodSandBox  91s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "a3f9991cbc3f69b961a0095e2c2bd1e804086124cc5a72e3dccabeb7efd071a2" network for pod "debug-default-67768587f8-7ck9h": networkPlugin cni failed to set up pod "debug-default-67768587f8-7ck9h_default" network: Failed in PollVm. Error : Failed HTTP Get operation. Return code 404
  Warning  FailedCreatePodSandBox  15s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "a15c13734087a871f0761c67bc2488c2c45f4596d3561b954e478c7a1b37913d" network for pod "debug-default-67768587f8-7ck9h": networkPlugin cni failed to set up pod "debug-default-67768587f8-7ck9h_default" network: Failed in PollVm. Error : Failed HTTP Get operation. Return code 404
  Normal   SandboxChanged          14s (x2 over 90s)  kubelet            Pod sandbox changed, it will be killed and re-created.

FW ? apparmour ?
should I add iptables rules
check any other multip node stack
disable auto scaling, drop nodes 2,3, does node 1 work ? 

https://stackoverflow.com/questions/60507920/i-am-getting-error-networkplugin-cni-failed-to-set-up-pod-on-deploying-pod

systemctl status kubelet
journalctl -u kubelet
I've managed to reproduce error that you encountered and it was happening when:
--pod-network-cidr=CIDR was not used with $ kubeadm init
Flannel CNI was applied after $ kubeadm init without --pod-network-cidr

https://kubernetes.feisky.xyz/v/en/index/network

no tap interfaces on any node.

---
https://github.com/tnaganawa/tungstenfabric-docs/blob/master/TungstenFabricPrimer.md

x. cni returned Poll VM-CFG 404 error

In kubernetes deployment, cni sometimes returns this error and won't assign IPs to pod. (This is seen in various place such kubectl describe pod)

networkPlugin cni failed to set up pod "coredns-5644d7b6d9-p8fkk_kube-system" network: Failed in Poll VM-CFG. Error : Failed in PollVM. Error : Failed HTTP Get operation. Return code 404
This message is a generic error, and caused by several reasons ..

Internally, when pod is created, cni tries to receive its IP from vrouter-agent, which in turn receive that from control process by XMPP.

that is based on virtual-machine-interface info, which is created by kube-manager from kube-apiserver info.
So to fix this issue, serveral steps need to be done.

contrail-status on controller node
config-api, control needs to be in ‘active’ state
contrail-status on contrail-kube-manager node is in 'active' state
this process will retrieve the info from kube-apiserver and create pod / load balancer etc on config-api
contrail-status on vrouter node
vrouter-agent needs to be in ‘active’ state
if standalone kubernetes yaml is used, it has known limitation about race condition between vrouter registration and vrouter-agent restart. Restarting control might resolve this issue.
 # docker restart control_control_1
if everything is fine,
/var/log/contrail/contrail-kube-manager.log
/var/log/contrail/api-zk.log
/var/log/contrail/contrail-vrouter-agent.log
/var/log/contrail/cni/opencontrail.log <- cni log
needs to be investigated further ..

root cause might be xmpp issue, underlay issue, /etc/hosts issue and so on

contrail-vrouter-nodemgr.log 
01/23/2021 02:14:42.131 7fda93021eb0 [contrail-vrouter-nodemgr] [ERROR]: Cannot write http_port 8102 to /tmp/contrail-vrouter-nodemgr.0.http_port


--------

cni no logs, is that normal

systemd-udevd[19177]: Could not generate persistent MAC address for tapeth0-aa5b5d: No such file or directory

vif --list shows no taps

try
sudo apt install ifupdown

I see taps on ip addr show, not on vif. 
redeply without ifupdown, using default netplan. do you see taps ? 

yes I see the tap's on ip addr show

port 5269 
control-node
5269 - XMPP port

this is controller to controller
 telnet 100.72.100.249 5269
Trying 100.72.100.249...
Connected to 100.72.100.249.
Escape character is '^]'.

this is worker to controller
telnet 100.72.100.249 5269
Trying 100.72.100.249...
Connected to 100.72.100.249.


lsof shows the server is up


lsof -i :8085
seems to be open, I can telnet




cat /etc/cni/net.d/10-contrail.conf 
{
    "cniVersion": "0.3.1",
    "contrail" : {
        "cluster-name"  : "k8s",
        "meta-plugin"   : "multus",
        "vrouter-ip"    : "127.0.0.1",
        "vrouter-port"  : 9091,
        "config-dir"    : "/var/lib/contrail/ports/vm",
        "poll-timeout"  : 5,
        "poll-retries"  : 15,
        "log-file"      : "/var/log/contrail/cni/opencontrail.log",
        "log-level"     : "4"
    },
    "name": "contrail-k8s-cni",
    "type": "contrail-k8s-cni"
}

ls /var/lib/contrail/ports/vm/
root@ip-100-72-100-29:~# 


I had this issue on aks when kubelet pod network was not the same as contrail
I am not seeing the pod network in kubelet, as a parameter..

if I cannot add a pod network to kubelet then whats the existing value, whats the default. 


I might be able to do it with 
   extra_args:
     max-pods: 150

we need this one --pod-cidr 
   extra_args:
     max-pods: 150
     pod-cidr: 10.32.0.0/24


the change worked in that kubelet now has option --pod-cidr
however still no overlay ip addresses. 

perhaps deplloy rke2 and llook for the cni logs as that one worked. 





 















