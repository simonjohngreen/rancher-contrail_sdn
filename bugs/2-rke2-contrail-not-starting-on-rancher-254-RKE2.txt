
kubectl version
Client Version: version.Info{Major:"", Minor:"", GitVersion:"v1.18.12+rke2r2", GitCommit:"7cd5e9086de8ae25d6a1514d0c87bac67ca4a481", GitTreeState:"clean", BuildDate:"2020-12-07T21:38:09Z", GoVersion:"go1.13.15b4", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.12+rke2r2", GitCommit:"7cd5e9086de8ae25d6a1514d0c87bac67ca4a481", GitTreeState:"clean", BuildDate:"2020-12-07T21:34:10Z", GoVersion:"go1.13.15b4", Compiler:"gc", Platform:"linux/amd64"}


crictl logs e2ac1a1e8a3cb
INFO: agent started in kernel mode
Device "vhost0" does not exist.
cat: /sys/class/net/vhost0/address: No such file or directory
INFO: creating vhost0 for nic mode: nic: ens5, mac=0a:5d:3e:e7:fe:29
              total        used        free      shared  buff/cache   available
Mem:            30G        5.6G         23G        3.0M        2.1G         25G
Swap:            0B          0B          0B
              total        used        free      shared  buff/cache   available
Mem:            30G        5.6G         23G        3.0M        2.0G         25G
Swap:            0B          0B          0B
INFO: load vrouter kernel module, options=''
modprobe: FATAL: Module vrouter not found.
WARNING: failed to load vrouter driver
vhost_create: Unknown error -95
Error registering NetLink client: No such file or directory (2)
Error registering NetLink client: No such file or directory (2)
Cannot find device "vhost0"
Cannot find device "vhost0"
cat: /sys/module/vrouter/version: No such file or directory
modinfo: ERROR: Module vrouter not found.
DEBUG: versions: agent=2008, loaded_vrouter=, available_vrouter=
FATAL: failed to init vhost0
root@ip-100-72-100-11:~# 
root@ip-100-72-100-11:~# 
root@ip-100-72-100-11:~# crictl ps --all | grep kernel
6f4c7aab8e4dc       2fe0af53b339e       40 minutes ago      Exited              contrail-vrouter-kernel-init             0                   7fff152a436f8
root@ip-100-72-100-11:~# crictl logs 6f4c7aab8e4dc
/etc/sysconfig/network-scripts /
/
Available vrouter.ko versions:
3.10.0-1062.4.1.el7.x86_64
3.10.0-1062.9.1.el7.x86_64
3.10.0-1062.12.1.el7.x86_64
Installed kernel versions:

root@ip-100-72-100-11:~# 
root@ip-100-72-100-11:~# uname -a
Linux ip-100-72-100-11 5.4.0-1029-aws #30~18.04.1-Ubuntu SMP Tue Oct 20 11:09:25 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux

kubectl get pods -A -o wide
NAMESPACE     NAME                                         READY   STATUS             RESTARTS   AGE   IP              NODE               NOMINATED NODE   READINESS GATES
kube-system   config-zookeeper-ps8s7                       1/1     Running            0          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-agent-wdgsk                         2/3     CrashLoopBackOff   7          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-analytics-alarm-xx8t9               3/4     CrashLoopBackOff   7          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-analytics-cqbgt                     3/4     CrashLoopBackOff   7          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-analytics-snmp-j2jp9                3/4     CrashLoopBackOff   7          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-analyticsdb-2sntk                   3/4     CrashLoopBackOff   7          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-configdb-npcqk                      2/3     CrashLoopBackOff   7          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-controller-config-796pw             5/6     CrashLoopBackOff   7          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-controller-control-7fgpb            4/5     CrashLoopBackOff   7          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-controller-webui-c8lht              2/2     Running            0          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   contrail-kube-manager-7787v                  1/1     Running            0          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   etcd-ip-100-72-100-11                        1/1     Running            0          19m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   helm-install-rancher-r9lmh                   0/1     CrashLoopBackOff   5          20m   10.32.0.252     ip-100-72-100-11   <none>           <none>
kube-system   helm-install-rke2-coredns-g8t78              0/1     Completed          0          20m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   helm-install-rke2-ingress-nginx-hwr6j        0/1     CrashLoopBackOff   5          20m   10.32.0.249     ip-100-72-100-11   <none>           <none>
kube-system   helm-install-rke2-kube-proxy-kwdxb           0/1     Completed          0          20m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   helm-install-rke2-metrics-server-nthdr       1/1     Running            6          20m   10.32.0.251     ip-100-72-100-11   <none>           <none>
kube-system   kube-apiserver-ip-100-72-100-11              1/1     Running            0          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   kube-controller-manager-ip-100-72-100-11     1/1     Running            0          20m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   kube-proxy-bfs6l                             1/1     Running            0          19m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   kube-scheduler-ip-100-72-100-11              1/1     Running            0          20m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   rabbitmq-lck6g                               1/1     Running            0          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   redis-tz4bx                                  1/1     Running            0          18m   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system   rke2-coredns-rke2-coredns-6f7676fdf7-rhnmb   0/1     ImagePullBackOff   0          19m   10.32.0.250     ip-100-72-100-11   <none>           <none>

vrouter is assigning ip's so thats good. 
But we have an issue with dns..

ping www.google.com
ping: www.google.com: Temporary failure in name resolution

kubelet --cluster-dns=10.43.0.10

and an issue with contrail-agent-nodemgr
which is keeping the vrouter down

==============================================
kubectl logs contrail-agent-wdgsk -n kube-system -c contrail-agent-nodemgr
INFO: wait for vhost0 is up
INFO: vhost0 is up
[DEFAULTS]
http_server_ip=0.0.0.0
log_file=/var/log/contrail/vrouter-nodemgr/contrail-vrouter-nodemgr.log
log_level=SYS_NOTICE
log_local=1
hostip=100.72.100.11
db_port=9042
db_jmx_port=7200
db_use_ssl=False

[COLLECTOR]
server_list=100.72.100.11:8086

[SANDESH]
introspect_ssl_enable=False
sandesh_ssl_enable=False


01/12/2021 19:34:19.152 7f7a33de1550 [contrail-vrouter-nodemgr] [INFO]: SANDESH: CONNECT TO COLLECTOR: True
01/12/2021 19:34:19.167 7f7a33de1550 [contrail-vrouter-nodemgr] [ERROR]: Failed to import package "vrouter.loadbalancer"
01/12/2021 19:34:19.173 7f7a33de1550 [contrail-vrouter-nodemgr] [INFO]: SANDESH: INTROSPECT IS ON: 0.0.0.0:8102
01/12/2021 19:34:19.177 7f7a33de1550 [contrail-vrouter-nodemgr] [ERROR]: Failed to import package "vrouter.loadbalancer"
01/12/2021 19:34:19.182 7f7a33de1550 [contrail-vrouter-nodemgr] [INFO]: SANDESH: Logging: LEVEL: [SYS_INFO] -> [SYS_NOTICE]
Traceback (most recent call last):
  File "/usr/bin/contrail-nodemgr", line 9, in <module>
    load_entry_point('nodemgr==0.1dev', 'console_scripts', 'contrail-nodemgr')()
  File "/usr/lib/python2.7/site-packages/nodemgr/main.py", line 242, in main
    event_manager = node_properties[node_type]['event_manager'](_args, unit_names)
  File "/usr/lib/python2.7/site-packages/nodemgr/vrouter_nodemgr/event_manager.py", line 22, in __init__
    config, type_info, unit_names, update_process_list=True)
  File "/usr/lib/python2.7/site-packages/nodemgr/common/event_manager.py", line 129, in __init__
    strategy = CriContainersInterface.craft_crio_peer()
  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 159, in craft_crio_peer
    return CriContainersInterface()._set_channel('/var/run/crio/crio.sock')
  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 167, in _set_channel
    raise LookupError(value_)
LookupError: /var/run/crio/crio.sock
==============================================

looks like all the nodemgr fail this way

kubectl logs contrail-controller-config-796pw -n kube-system -c contrail-controller-config-nodemgr
[DEFAULTS]
minimum_diskGB = 2
01/12/2021 20:14:14.102 7f653fdda550 [contrail-config-nodemgr] [INFO]: SANDESH: CONNECT TO COLLECTOR: True
01/12/2021 20:14:14.123 7f653fdda550 [contrail-config-nodemgr] [INFO]: SANDESH: INTROSPECT IS ON: 0.0.0.0:8100
01/12/2021 20:14:14.132 7f653fdda550 [contrail-config-nodemgr] [INFO]: SANDESH: Logging: LEVEL: [SYS_INFO] -> [SYS_NOTICE]
Traceback (most recent call last):
  File "/usr/bin/contrail-nodemgr", line 9, in <module>
    load_entry_point('nodemgr==0.1dev', 'console_scripts', 'contrail-nodemgr')()
  File "/usr/lib/python2.7/site-packages/nodemgr/main.py", line 242, in main
    event_manager = node_properties[node_type]['event_manager'](_args, unit_names)
  File "/usr/lib/python2.7/site-packages/nodemgr/config_nodemgr/event_manager.py", line 19, in __init__
    super(ConfigEventManager, self).__init__(config, type_info, unit_names)
  File "/usr/lib/python2.7/site-packages/nodemgr/common/event_manager.py", line 129, in __init__
    strategy = CriContainersInterface.craft_crio_peer()
  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 159, in craft_crio_peer
    return CriContainersInterface()._set_channel('/var/run/crio/crio.sock')
  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 167, in _set_channel
    raise LookupError(value_)
LookupError: /var/run/crio/crio.sock



why is it looking for /var/run/crio/crio.sock 
crio is RHEL openshift.

we are /var/run/containerd

https://github.com/cri-o/cri-o/issues/3791
sudo systemctl status crio
Unit crio.service could not be found.

we are running cri with containerd not with cri-o
check the release notes.

openshift uses cri-o


https://jira.tungsten.io/projects/TFP/issues/TFP-94?filter=allopenissues



so lets check the stack..
Traceback (most recent call last):
  File "/usr/bin/contrail-nodemgr", line 9, in <module>
    load_entry_point('nodemgr==0.1dev', 'console_scripts', 'contrail-nodemgr')()
  File "/usr/lib/python2.7/site-packages/nodemgr/main.py", line 242, in main
    event_manager = node_properties[node_type]['event_manager'](_args, unit_names)
  File "/usr/lib/python2.7/site-packages/nodemgr/vrouter_nodemgr/event_manager.py", line 22, in __init__
    config, type_info, unit_names, update_process_list=True)
  File "/usr/lib/python2.7/site-packages/nodemgr/common/event_manager.py", line 129, in __init__
    strategy = CriContainersInterface.craft_crio_peer()
  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 159, in craft_crio_peer
    return CriContainersInterface()._set_channel('/var/run/crio/crio.sock')
  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 167, in _set_channel
    raise LookupError(value_)
LookupError: /var/run/crio/crio.sock



lets take a look at the code. 



crictl ps --all | grep nodemgr
68a91c9041023       77faf792ed4e3       Less than a second ago   Running             contrail-analytics-nodemgr               17                  ff2868dfd0c14
f22059ce7b5c9       77faf792ed4e3       6 seconds ago            Exited              contrail-analytics-snmp-nodemgr          17                  8dea12d9e0ddc
41112ede8aae3       77faf792ed4e3       3 minutes ago            Exited              contrail-agent-nodemgr                   16                  78d5150dfe64c
af603215145e4       77faf792ed4e3       4 minutes ago            Exited              contrail-analyticsdb-nodemgr             16                  bd4a84e2c68bb
ca2d96bc84715       77faf792ed4e3       4 minutes ago            Exited              contrail-analytics-alarm-nodemgr         16                  e2062974d798a
1d3b4765fea48       77faf792ed4e3       4 minutes ago            Exited              contrail-controller-config-nodemgr       16                  ab5960e981f06
3b05e3b41acfe       77faf792ed4e3       4 minutes ago            Exited              contrail-controller-nodemgr              16                  e7e9a8e495c3f
11fa14a03a1c9       77faf792ed4e3       4 minutes ago            Exited              contrail-config-database-nodemgr         16                  80b3dbe9f6108
67d2b2dd7c49f       77faf792ed4e3       5 minutes ago            Exited              contrail-analytics-nodemgr               16                  ff2868dfd0c14


crictl isn't great for looking at docker iamges, so lets drop to my mac to look at the code. 

docker login xxxx yyyy
docker image pull hub.juniper.net/contrail/contrail-nodemgr:2011.138
docker image list | grep nodemgr
hub.juniper.net/contrail/contrail-nodemgr               2011.138                                         77faf792ed4e   3 weeks ago     720MB
docker run -it --entrypoint /bin/bash 77faf792ed4e

  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 167, in _set_channel
    raise LookupError(value_)


I see this code in here..

class CriContainersInterface:
    @staticmethod
    def craft_crio_peer():
        return CriContainersInterface()._set_channel('/var/run/crio/crio.sock')
    @staticmethod
    def craft_containerd_peer():
        return CriContainersInterface()._set_channel('/run/containerd/containerd.sock')

so we think we can at least do containerd

cd /usr/lib/python2.7/site-packages/nodemgr/common

utils.py
def is_running_in_docker():
    return os.path.exists('/.dockerenv')
def is_running_in_podman():
    pid = os.getpid()
    with open('/proc/{}/cgroup'.format(pid), 'rt') as ifh:
        return 'libpod' in ifh.read()
def is_running_in_kubepod():
    pid = os.getpid()
    with open('/proc/{}/cgroup'.format(pid), 'rt') as ifh:
        return 'kubepods' in ifh.read()
def is_running_in_containerd():
    pid = os.getpid()
    with open('/proc/{}/cgroup'.format(pid), 'rt') as ifh:
        return 'containerd' in ifh.read()

cri_containers.py
class CriContainersInterface:
    @staticmethod
    def craft_crio_peer():
        return CriContainersInterface()._set_channel('/var/run/crio/crio.sock')
    @staticmethod
    def craft_containerd_peer():
        return CriContainersInterface()._set_channel('/run/containerd/containerd.sock')

event_manager.py
        if ContainerProcessInfoManager:
            strategy = None
            if utils.is_running_in_docker():
                strategy = DockerContainersInterface()
            elif utils.is_running_in_podman():
                strategy = PodmanContainersInterface()
            elif utils.is_running_in_kubepod():
                strategy = CriContainersInterface.craft_crio_peer()
            elif utils.is_running_in_containerd():
                strategy = CriContainersInterface.craft_containerd_peer()


so we are hittong this one and setting up cri-o
def is_running_in_kubepod():    
    pid = os.getpid()
    with open('/proc/{}/cgroup'.format(pid), 'rt') as ifh:
        return 'kubepods' in ifh.read()

looking at my host.. this is how /proc cgroups looks
cat ./18667/cgroup
12:cpuset:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
11:devices:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
10:net_cls,net_prio:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
9:memory:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
8:pids:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
7:freezer:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
6:rdma:/
5:blkio:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
4:hugetlb:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
3:perf_event:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
2:cpu,cpuacct:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
1:name=systemd:/kubepods/besteffort/pod1663966f-ce7a-4f47-b1f9-23154742df27/97c62d02caf7a682c8c1881c1d489e4d55ae2e8f7e2fddcf948fc4e1c2995d66
0::/system.slice/rancherd-server.service

for i in `find . | grep "[0-9]/cgroup"`; do echo $i; grep kubepods $i ; done
lots of hits..
for i in `find . | grep "[0-9]/cgroup"`; do echo $i; grep containerd $i ; done
no hits
for i in `find . | grep "[0-9]/cgroup"`; do echo $i; grep rancherd-server $i ; done

my containerd config is here /var/lib/rancher/rke2/agent/etc/containerd/config.toml, nothing unusual
[plugins.opt]
  path = "/var/lib/rancher/rke2/agent/containerd"
[plugins.cri]
  stream_server_address = "127.0.0.1"
  stream_server_port = "10010"
  enable_selinux = false
  sandbox_image = "docker.io/rancher/pause:3.2"
[plugins.cri.containerd]
  snapshotter = "overlayfs"
[plugins.cri.containerd.runtimes.runc]
  runtime_type = "io.containerd.runc.v2"

as a quick fix/workarund for the customer lets update the following in a private image 
cri_containers.py
    def craft_crio_peer():
        return CriContainersInterface()._set_channel('/var/run/crio/crio.sock')
to 
    def craft_crio_peer():
        return CriContainersInterface()._set_channel('/run/k3s/containerd/containerd.sock')

********
docker login hub.juniper.net --username xxxxxx --password xxxxxx
docker run -ti --entrypoint=/bin/bash hub.juniper.net/contrail/contrail-nodemgr:2011.138 

#note the below patch uses /mnt because /mnt in the container is actualy /var/run on the host (makes me wonder how the rest of the code ever worked)
edit /usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py
    @staticmethod
    def craft_crio_peer():
--      return CriContainersInterface()._set_channel('/var/run/crio/crio.sock')
++      return CriContainersInterface()._set_channel('/mnt/k3s/containerd/containerd.sock')

in another window, on the host, commit the image to the same tag , just for a quick test.
docker ps | grep [kube-manager image id]
f19bba9fe99d   77faf792ed4e   "/bin/bash"              About an hour ago   Up About an hour             gifted_mendel

docker commit --change='ENTRYPOINT ["/entrypoint.sh", "/bin/sh", "-c", "/usr/bin/contrail-nodemgr --nodetype=${NODEMGR_TYPE}"]' f19bba9fe99d 

docker image list  | head
<none>                                                  <none>                                           b2c16ec1382f   12 seconds ago   720MB
#push the patched image up to docker hub
docker tag b2c16ec1382f docker.io/simonjohngreen/contrail-nodemgr:2011.138-patchv1
docker image list  | head
simonjohngreen/contrail-nodemgr                         2011.138-patchv1                                 b2c16ec1382f   3 minutes ago   720MB
docker logout hub.juniper.net
docker login docker.io
docker push docker.io/simonjohngreen/contrail-nodemgr:2011.138-patchv1

now edit the s3 contrail-rancher.yaml, all occurances 
--         image: "hub.juniper.net/contrail/contrail-nodemgr:[RELEASE]" 
++         image: docker.io/simonjohngreen/contrail-nodemgr:2011.138-patchv1

redeploy. 
didn't work nodemgr now exits silently.
setup SYS_DEBUG

I get a little more debug, but it still dies silently

[SANDESH]
introspect_ssl_enable=False
sandesh_ssl_enable=False

.
.
+ upgrade_old_logs contrail-vrouter-nodemgr
+ local template=contrail-vrouter-nodemgr
++ dirname /var/log/contrail/vrouter-nodemgr
+ local old_dir=/var/log/contrail
+ mkdir -p /var/log/contrail/vrouter-nodemgr
+ chmod 755 /var/log/contrail/vrouter-nodemgr
+ mv -n '/var/log/contrail/*contrail-vrouter-nodemgr.log*' /var/log/contrail/vrouter-nodemgr/
+ true
+ [[ -n 1999 ]]
+ [[ -n 1999 ]]
+ local owner_opts=1999:1999
+ chown 1999:1999 /var/log/contrail/vrouter-nodemgr
+ find /var/log/contrail/vrouter-nodemgr -uid 0 -exec chown 1999:1999 '{}' +
+ exec

double checking the crictl inspect I see a different, in the pre-patched..

        "args": [
          "/entrypoint.sh",
          "/bin/sh",
          "-c",
          "/usr/bin/contrail-nodemgr --nodetype=${NODEMGR_TYPE}"
        ],

rework the patch and try again

new error
  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 167, in _set_channel
    raise LookupError(value_)
LookupError: /var/run/k3s/containerd/containerd.sock

check the command..

        "args": [
          "/entrypoint.sh",
          "/bin/sh",
          "-c",
          "/usr/bin/contrail-nodemgr --nodetype=${NODEMGR_TYPE}"
        ],

ok so try it without the /var    just /run/k3s/containerd/containerd.sock this is how kubelet addresses it 

01/13/2021 23:16:52.284 7ff98e5dc550 [contrail-vrouter-nodemgr] [INFO]: SANDESH: CONNECT TO COLLECTOR: True
01/13/2021 23:16:52.299 7ff98e5dc550 [contrail-vrouter-nodemgr] [ERROR]: Failed to import package "vrouter.loadbalancer"
01/13/2021 23:16:52.304 7ff98e5dc550 [contrail-vrouter-nodemgr] [INFO]: SANDESH: INTROSPECT IS ON: 0.0.0.0:8102
01/13/2021 23:16:52.308 7ff98e5dc550 [contrail-vrouter-nodemgr] [ERROR]: Failed to import package "vrouter.loadbalancer"
01/13/2021 23:16:52.313 7ff98e5dc550 [contrail-vrouter-nodemgr] [INFO]: SANDESH: Logging: LEVEL: [SYS_INFO] -> [SYS_DEBUG]
01/13/2021 23:16:52.313 7ff98e5dc550 [contrail-vrouter-nodemgr] [INFO]: SANDESH: Logging: FILE: [None] -> [/var/log/contrail/vrouter-nodemgr/contrail-vrouter-nodemgr.log]
Traceback (most recent call last):
  File "/usr/bin/contrail-nodemgr", line 9, in <module>
    load_entry_point('nodemgr==0.1dev', 'console_scripts', 'contrail-nodemgr')()
  File "/usr/lib/python2.7/site-packages/nodemgr/main.py", line 242, in main
    event_manager = node_properties[node_type]['event_manager'](_args, unit_names)
  File "/usr/lib/python2.7/site-packages/nodemgr/vrouter_nodemgr/event_manager.py", line 22, in __init__
    config, type_info, unit_names, update_process_list=True)
  File "/usr/lib/python2.7/site-packages/nodemgr/common/event_manager.py", line 129, in __init__
    strategy = CriContainersInterface.craft_crio_peer()
  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 159, in craft_crio_peer
    return CriContainersInterface()._set_channel('/run/k3s/containerd/containerd.sock')
  File "/usr/lib/python2.7/site-packages/nodemgr/common/cri_containers.py", line 167, in _set_channel
    raise LookupError(value_)
LookupError: /run/k3s/containerd/containerd.sock

check the mounts

          "destination": "/mnt",
          "type": "bind",
          "source": "/run",
          "options": [
            "rbind",
            "rprivate",
            "rw"

        "containerPath": "/mnt",
        "hostPath": "/var/run",

so lets try /mnt/k3s/containerd/containerd.sock
yes that worked, contrail is all up. 

kubectl get pods -A -o wide
NAMESPACE       NAME                                                  READY   STATUS             RESTARTS   AGE     IP              NODE               NOMINATED NODE   READINESS GATES
cattle-system   rancher-lqsrw                                         0/1     ImagePullBackOff   0          3m      10.32.0.248     ip-100-72-100-11   <none>           <none>
kube-system     config-zookeeper-vjz4p                                1/1     Running            0          7m24s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-agent-c56w7                                  3/3     Running            0          7m22s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-analytics-5jmqj                              4/4     Running            0          7m24s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-analytics-alarm-vfsn7                        4/4     Running            0          7m24s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-analytics-snmp-prkql                         4/4     Running            0          7m24s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-analyticsdb-ncvxt                            4/4     Running            0          7m24s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-configdb-2wvvq                               3/3     Running            0          7m24s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-controller-config-nfzfm                      6/6     Running            0          7m23s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-controller-control-kznpt                     5/5     Running            0          7m24s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-controller-webui-mm9j2                       2/2     Running            0          7m23s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     contrail-kube-manager-dzjdm                           1/1     Running            0          7m22s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     etcd-ip-100-72-100-11                                 1/1     Running            0          8m10s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     helm-install-rancher-zjgcd                            0/1     Completed          0          8m53s   10.32.0.251     ip-100-72-100-11   <none>           <none>
kube-system     helm-install-rke2-ingress-nginx-rpwlh                 0/1     Completed          0          8m53s   10.32.0.252     ip-100-72-100-11   <none>           <none>
kube-system     helm-install-rke2-kube-proxy-lgxn6                    0/1     Completed          0          8m53s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     helm-install-rke2-metrics-server-xxlm7                0/1     Completed          0          8m53s   10.32.0.250     ip-100-72-100-11   <none>           <none>
kube-system     kube-apiserver-ip-100-72-100-11                       1/1     Running            0          7m56s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     kube-controller-manager-ip-100-72-100-11              1/1     Running            0          9m7s    100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     kube-proxy-qc57q                                      1/1     Running            0          8m36s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     kube-scheduler-ip-100-72-100-11                       1/1     Running            0          9m7s    100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     rabbitmq-nsg94                                        1/1     Running            0          7m23s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     redis-dnlf7                                           1/1     Running            0          7m23s   100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     rke2-coredns-rke2-coredns-6f7676fdf7-l2cl7            0/1     ImagePullBackOff   0          4m39s   10.32.0.249     ip-100-72-100-11   <none>           <none>
kube-system     rke2-ingress-nginx-controller-d4989f458-9fwpp         0/1     ImagePullBackOff   0          3m      100.72.100.11   ip-100-72-100-11   <none>           <none>
kube-system     rke2-ingress-nginx-default-backend-65f75d6664-wngr5   0/1     ImagePullBackOff   0          3m      10.32.0.246     ip-100-72-100-11   <none>           <none>
kube-system     rke2-metrics-server-5d8c549c9f-4dprt                  0/1     ImagePullBackOff   0          3m      10.32.0.247     ip-100-72-100-11   <none>           <none>

*****


so conclusion is ,. untill we get support for embedded containerd as the runtime in contrail, RKE2 seems to be off the cards. 
=============================================
Rancher is moving towards deeplloying with RKE2 installed with RancherD
My cusotmer is looking to go into production with Rancher mid year and has asked us to integrate contrai as the CNI
RKE2 uses an embeeddd containerd as the kubernetees cri. 
Deploying contrial as the CNI into Rancher 2.5.4 RKE2, the  nodemgr containers all fail, RKE2 runs embedded containerd, contrail nodemgr  mistakenly thinks its cri-o so sets up the wrong runtime. 

def is_running_in_kubepod():
    pid = os.getpid()
    with open('/proc/{}/cgroup'.format(pid), 'rt') as ifh:
        return 'kubepods' in ifh.read()

so its setup as cri-o and fails as the runtime is infact here.. /run/k3s/containerd/containerd.sock

so I've added a new check for embedded containerd on rancher

We have a customer looking to run Contrail as the CNI on Rancher within production.
We broadly say we can integrate contrail into Rancher using null cni. This appears to no longer be the case with the introduction of RKE2 which is running containerD.
=============================================





 




